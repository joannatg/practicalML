library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(caret)
library(e1071)
library(randomForest)
library(corrplot)
library(gbm)


getwd()
setwd("C:\\Users\\Joanna\\Desktop\\R_coursera\\practicalML\\")

##https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
train_data <- read.csv('pml-training.csv', na.strings = c("NA", "#DIV/0!", ""))


##https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
test_data <- read.csv('pml-testing.csv', na.strings = c("NA", "#DIV/0!", ""))

##Remove feature with missing values
clnColumnIndex <- colSums(is.na(train_data))/nrow(train_data) < 0.95
clean_train_data <- train_data[,clnColumnIndex]
colSums(is.na(clean_train_data))/nrow(clean_train_data)
colSums(is.na(clean_train_data))

## Remove irrelevant features (1-7)
clean_train_data <- clean_train_data[,-c(1:7)]
clean_val_data <- test_data[,-c(1:7)]

##Split into train (70%) and test (30%) sets
inTrInd <- createDataPartition(clean_train_data$classe, p=0.7)[[1]]
train_set <- clean_train_data[inTrInd,]
val_set <- clean_train_data[-inTrInd,]
##all_names <- names(clean_train_data)
##clean_test_data <- test_data[,all_names[1:52]]
head(train_set)
head(val_set)


##Correlation between features 
correlationMatrix <-cor(data.matrix(train_set)[, -54])
correlationMatrix
corrplot(correlationMatrix, order="FPC", method="circle", type="full", tl.cex=0.7,  tl.col="black", tl.srt=45, col=brewer.pal(n=8, name="RdYlBu")) ##(fig1)

## Algorithm 1: Decision tree 
set.seed(12345)
dTreeMod <- rpart(classe ~ ., data=train_set, method="class")
fancyRpartPlot(dTreeMod) ##(fig2)

##Validate dTreeMod using the clean test data
valdTreeMod <- predict(dTreeMod, val_set, type = "class")
vdTreeMod <- confusionMatrix(valdTreeMod, val_set$classe)
vdTreeMod
#Decision tree accuracy = 0.75


##Algorithm 2: Random forest
set.seed(12345)
rforestMod <- train(classe ~., method='rf', data=train_set, ntree=128)
rforestMod$finalModel

##Validate rforestMod using the clean test data
valrforestMod <- predict(rforestMod , newdata=val_set)
crforest <- confusionMatrix(valrforestMod, val_set$classe)
crforest
##Random Forest accuracy = 0.99


## Use the model with higher accuracy - random forest, on the test dataset
finalResult <- predict(rforestMod, newdata=clean_val_data )
finalResult



